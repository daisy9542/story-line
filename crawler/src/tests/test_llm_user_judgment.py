import asyncio
import json
from src.utils import call_llm

TESTSET_PATH = "data/llm_user_relevance_eval.jsonl"


def load_testset(path: str) -> list:
    with open(path, "r", encoding="utf-8") as f:
        return [json.loads(line) for line in f]


def build_prompt(bio: str, tweets: list[str]) -> str:
    return f"""è¯·åˆ¤æ–­æ­¤ç”¨æˆ·æ˜¯å¦ä»äº‹ä¸åŒºå—é“¾ / Web3 / åŠ å¯†è´§å¸ç›¸å…³çš„å·¥ä½œæˆ–ç ”ç©¶ã€‚å¦‚æœç›¸å…³ï¼Œä»…è¿”å› Trueï¼Œå¦åˆ™ä»…è¿”å› Falseã€‚

ã€ç”¨æˆ·ç®€ä»‹ã€‘ï¼š
{bio}

ã€æœ€è¿‘æ¨æ–‡ã€‘ï¼š
{chr(10).join(f'- {t}' for t in tweets)}

è¯·ä½ åªè¿”å› True æˆ– Falseã€‚
"""


async def run_testset():
    samples = load_testset(TESTSET_PATH)
    passed = 0
    failed = 0
    print(f"ğŸ§ª æ­£åœ¨æµ‹è¯• {len(samples)} æ¡æ ·ä¾‹...\n")
    
    for i, sample in enumerate(samples):
        prompt = build_prompt(sample["bio"], sample["tweets"])
        try:
            result = await call_llm(prompt)
            pred = result.strip().lower() == "true"
        except Exception as e:
            print(f"âŒ æ ·ä¾‹ {i + 1} è°ƒç”¨å¤±è´¥ï¼š{e}")
            failed += 1
            continue
        
        expected = sample["expected"]
        if pred == expected:
            print(f"âœ… æ ·ä¾‹ {i + 1} é€šè¿‡")
            passed += 1
        else:
            print(f"âŒ æ ·ä¾‹ {i + 1} é”™è¯¯ï¼Œé¢„æµ‹: {pred}ï¼Œåº”ä¸º: {expected}")
            failed += 1
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼é€šè¿‡: {passed}ï¼Œå¤±è´¥: {failed}ï¼Œå‡†ç¡®ç‡: {passed / len(samples):.2%}")


if __name__ == "__main__":
    asyncio.run(run_testset())
